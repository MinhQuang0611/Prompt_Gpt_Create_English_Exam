{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "checking text : \n",
      "The deadlist virus in modern history, perhaps of all time, was the 1918 Spanish Flu. It killed about 20 to 50 million people worldwide, perhaps more. The total death toll is unknown because medical records were not kept in many areas.\n",
      "The pandemic hit during World War I and devastated military troops. In the United States, for instance, more servicemen were killed from the flu than from the war itself. The Spanish flu was fatal to a higher proportion of young adults than most flu viruses.\n",
      "The pandemic started mildly, in the spring of 1918, but was followed by a much more severe wave in the fall of 1918. The war likely contributed to the devastating mortality numbers, as large outbreaks occurred in military forces living in close quarters. Poor nutrition and the unsanitary conditions of war camps had an effect.\n",
      "A third wave occurred in the winter and spring of 1919, and a fourth, smaller wave occurred in a few areas in spring 1920. Initial symptoms of the flu were typical: sore throat, headache, and fever. The flu often progressed rapidly to cause severe pneumonia and sometimes hemorrhage in the lungs and mucus membranes. A characteristic feature of severe cases of the Spanish Flu was heliotrope cyanosis, where the patient’s face turned blue from lack of oxygen in the cells. Death usually followed within hours or days.\n",
      "Modern medicine such as vaccines, antivirals, and antibiotics for secondary infections were not available at that time, so medical personnel couldn’t do much more than try to relieve symptoms.\n",
      "The flu ended when it had infected enough people that those who were susceptible had either died or developed immunity.\n",
      "\n",
      "\n",
      " KET QUA NHAN DUOC :\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading LanguageTool 6.4: 100%|██████████| 246M/246M [00:05<00:00, 41.4MB/s] \n",
      "Unzipping C:\\Users\\LEGION\\AppData\\Local\\Temp\\tmpjstlrd4h.zip to C:\\Users\\LEGION\\.cache\\language_tool_python.\n",
      "Downloaded https://www.languagetool.org/download/LanguageTool-6.4.zip to C:\\Users\\LEGION\\.cache\\language_tool_python.\n"
     ]
    }
   ],
   "source": [
    "import language_tool_python\n",
    "\n",
    "def check_text(text):\n",
    "\n",
    "    tool = language_tool_python.LanguageTool('en-US')\n",
    "\n",
    "    matches = tool.check(text)\n",
    "    issues = []\n",
    "    for match in matches:\n",
    "        issue = {\n",
    "            'message': match.message,\n",
    "            'context': match.context,\n",
    "            'offset': match.offset,\n",
    "            'length': match.errorLength,\n",
    "            'category': match.category,\n",
    "            'rule_id': match.ruleId,\n",
    "            'suggestions': match.replacements\n",
    "        \n",
    "        }\n",
    "        issues.append(issue)\n",
    "    tool.close()\n",
    "\n",
    "    return issues\n",
    "\n",
    "def print_issue(text):\n",
    "    issues = check_text(text)\n",
    " \n",
    "    if not issues:\n",
    "        print(\"no issue found\")\n",
    "        return \n",
    "    else:\n",
    "        print(f\"found {len(issues)} issues\")\n",
    "\n",
    "        for i, issue in enumerate(issues, 1):\n",
    "            print(f\"issue :{i}\")\n",
    "            print(f\"context: {issue['message']}\")\n",
    "            if issue['suggestions']:\n",
    "                print(f\"suggestions: {issue['suggestions']}\")\n",
    "            print()\n",
    "        \n",
    "\n",
    "text = \"\"\"The deadlist virus in modern history, perhaps of all time, was the 1918 Spanish Flu. It killed about 20 to 50 million people worldwide, perhaps more. The total death toll is unknown because medical records were not kept in many areas.\n",
    "The pandemic hit during World War I and devastated military troops. In the United States, for instance, more servicemen were killed from the flu than from the war itself. The Spanish flu was fatal to a higher proportion of young adults than most flu viruses.\n",
    "The pandemic started mildly, in the spring of 1918, but was followed by a much more severe wave in the fall of 1918. The war likely contributed to the devastating mortality numbers, as large outbreaks occurred in military forces living in close quarters. Poor nutrition and the unsanitary conditions of war camps had an effect.\n",
    "A third wave occurred in the winter and spring of 1919, and a fourth, smaller wave occurred in a few areas in spring 1920. Initial symptoms of the flu were typical: sore throat, headache, and fever. The flu often progressed rapidly to cause severe pneumonia and sometimes hemorrhage in the lungs and mucus membranes. A characteristic feature of severe cases of the Spanish Flu was heliotrope cyanosis, where the patient’s face turned blue from lack of oxygen in the cells. Death usually followed within hours or days.\n",
    "Modern medicine such as vaccines, antivirals, and antibiotics for secondary infections were not available at that time, so medical personnel couldn’t do much more than try to relieve symptoms.\n",
    "The flu ended when it had infected enough people that those who were susceptible had either died or developed immunity.\n",
    "\"\"\"\n",
    "print(\"checking text : \" )\n",
    "print(text)\n",
    "print(\"\\n KET QUA NHAN DUOC :\")\n",
    "print_issue(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking text:\n",
      "The deadlist virus in modern history, perhaps of all time, was the 1918 Spanish Flu. It killed about 20 to 50 million people worldwide, perhaps more. The total death toll is unknown because medical records were not kept in many areas.\n",
      "The pandemic hit during World War I and devastated military troops. In the United States, for instance, more servicemen were killed from the flu than from the war itself. The Spanish flu was fatal to a higher proportion of young adults than most flu viruses.\n",
      "The pandemic started mildly, in the spring of 1918, but was followed by a much more severe wave in the fall of 1918. The war likely contributed to the devastating mortality numbers, as large outbreaks occurred in military forces living in close quarters. Poor nutrition and the unsanitary conditions of war camps had an effect.\n",
      "A third wave occurred in the winter and spring of 1919, and a fourth, smaller wave occurred in a few areas in spring 1920. Initial symptoms of the flu were typical: sore throat, headache, and fever. The flu often progressed rapidly to cause severe pneumonia and sometimes hemorrhage in the lungs and mucus membranes. A characteristic feature of severe cases of the Spanish Flu was heliotrope cyanosis, where the patient's face turned blue from lack of oxygen in the cells. Death usually followed within hours or days.\n",
      "Modern medicine such as vaccines, antivirals, and antibiotics for secondary infections were not available at that time, so medical personnel couldn't do much more than try to relieve symptoms.\n",
      "The flu ended when it had infected enough people that those who were susceptible had either died or developed immunity.\n",
      "\n",
      "Results:\n",
      "Found 1 spelling mistakes:\n",
      "\n",
      "Mistake #1:\n",
      "- Word: deadlist\n",
      "- Suggestions: deadliest\n",
      "- Position(s): word #2\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from spellchecker import SpellChecker\n",
    "\n",
    "class SpellingChecker:\n",
    "    def __init__(self):\n",
    "        self.spell = SpellChecker()\n",
    "    \n",
    "    def check_text(self, text):\n",
    "        \"\"\"\n",
    "        Check spelling in a text and return corrections\n",
    "        \"\"\"\n",
    "        # Split text into words and keep track of their positions\n",
    "        words = text.split()\n",
    "        word_positions = {}\n",
    "        \n",
    "        # Store positions for each word\n",
    "        for index, word in enumerate(words):\n",
    "            # Remove punctuation from word for checking\n",
    "            clean_word = word.strip('.,!?:;()[]{}\"\"''')\n",
    "            if clean_word not in word_positions:\n",
    "                word_positions[clean_word] = []\n",
    "            word_positions[clean_word].append(index)\n",
    "        \n",
    "        # Find misspelled words\n",
    "        misspelled = self.spell.unknown([word.strip('.,!?:;()[]{}\"\"''') for word in words])\n",
    "        \n",
    "        # Store corrections\n",
    "        corrections = []\n",
    "        for word in misspelled:\n",
    "            # Get all positions for this misspelled word\n",
    "            positions = word_positions.get(word, [])\n",
    "            \n",
    "            correction = {\n",
    "                'word': word,\n",
    "                'suggestions': list(self.spell.candidates(word)),\n",
    "                'positions': positions  # Now storing all positions\n",
    "            }\n",
    "            corrections.append(correction)\n",
    "            \n",
    "        return corrections\n",
    "\n",
    "    def print_corrections(self, text):\n",
    "        \"\"\"\n",
    "        Print spelling corrections in a formatted way\n",
    "        \"\"\"\n",
    "        try:\n",
    "            corrections = self.check_text(text)\n",
    "            \n",
    "            if not corrections:\n",
    "                print(\"No spelling mistakes found!\")\n",
    "                return\n",
    "            \n",
    "            print(f\"Found {len(corrections)} spelling mistakes:\\n\")\n",
    "            \n",
    "            for i, correction in enumerate(corrections, 1):\n",
    "                print(f\"Mistake #{i}:\")\n",
    "                print(f\"- Word: {correction['word']}\")\n",
    "                print(f\"- Suggestions: {', '.join(correction['suggestions'][:5])}\")\n",
    "                # Print all positions where the word appears\n",
    "                positions_str = ', '.join(str(pos + 1) for pos in correction['positions'])\n",
    "                print(f\"- Position(s): word #{positions_str}\")\n",
    "                print()\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"An error occurred while checking the text: {str(e)}\")\n",
    "\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    checker = SpellingChecker()\n",
    "    \n",
    "    test_text = \"\"\"The deadlist virus in modern history, perhaps of all time, was the 1918 Spanish Flu. It killed about 20 to 50 million people worldwide, perhaps more. The total death toll is unknown because medical records were not kept in many areas.\n",
    "The pandemic hit during World War I and devastated military troops. In the United States, for instance, more servicemen were killed from the flu than from the war itself. The Spanish flu was fatal to a higher proportion of young adults than most flu viruses.\n",
    "The pandemic started mildly, in the spring of 1918, but was followed by a much more severe wave in the fall of 1918. The war likely contributed to the devastating mortality numbers, as large outbreaks occurred in military forces living in close quarters. Poor nutrition and the unsanitary conditions of war camps had an effect.\n",
    "A third wave occurred in the winter and spring of 1919, and a fourth, smaller wave occurred in a few areas in spring 1920. Initial symptoms of the flu were typical: sore throat, headache, and fever. The flu often progressed rapidly to cause severe pneumonia and sometimes hemorrhage in the lungs and mucus membranes. A characteristic feature of severe cases of the Spanish Flu was heliotrope cyanosis, where the patient's face turned blue from lack of oxygen in the cells. Death usually followed within hours or days.\n",
    "Modern medicine such as vaccines, antivirals, and antibiotics for secondary infections were not available at that time, so medical personnel couldn't do much more than try to relieve symptoms.\n",
    "The flu ended when it had infected enough people that those who were susceptible had either died or developed immunity.\"\"\"\n",
    "    \n",
    "    print(\"Checking text:\")\n",
    "    print(test_text)\n",
    "    print(\"\\nResults:\")\n",
    "    checker.print_corrections(test_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Error loading punkt: <urlopen error [WinError 10060] A\n",
      "[nltk_data]     connection attempt failed because the connected party\n",
      "[nltk_data]     did not properly respond after a period of time, or\n",
      "[nltk_data]     established connection failed because connected host\n",
      "[nltk_data]     has failed to respond>\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\Thanh Minh\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking text:\n",
      "The deadlist virus in modern history, perhaps of all time, was the 1918 Spanish Flu. \n",
      "    It killed about 20 to 50 million people worldwide, perhaps more. \n",
      "    The total death toll is unknown because medical records were not kept in many areas.\n",
      "\n",
      "Results:\n",
      "An error occurred while checking the text: \n",
      "Looks like you are missing some required data for this feature.\n",
      "\n",
      "To download the necessary data, simply run\n",
      "\n",
      "    python -m textblob.download_corpora\n",
      "\n",
      "or use the NLTK downloader to download the missing data: http://nltk.org/data.html\n",
      "If this doesn't fix the problem, file an issue at https://github.com/sloria/TextBlob/issues.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from textblob import TextBlob\n",
    "import re\n",
    "\n",
    "class TextBlobChecker:\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    \n",
    "    def check_text(self, text):\n",
    "        \"\"\"\n",
    "        Check text using TextBlob with enhanced error detection\n",
    "        \"\"\"\n",
    "        blob = TextBlob(text)\n",
    "        \n",
    "        # Split into sentences for better analysis\n",
    "        sentences = text.split('.')\n",
    "        corrections = []\n",
    "        \n",
    "        # Process each sentence\n",
    "        word_position = 1  # Keep track of global word position\n",
    "        \n",
    "        for sentence in sentences:\n",
    "            if not sentence.strip():\n",
    "                continue\n",
    "                \n",
    "            # Create TextBlob for this sentence\n",
    "            sentence_blob = TextBlob(sentence)\n",
    "            \n",
    "            # Get original and corrected words\n",
    "            original_words = re.findall(r'\\b\\w+\\b', sentence)\n",
    "            corrected_words = re.findall(r'\\b\\w+\\b', str(sentence_blob.correct()))\n",
    "            \n",
    "            # Compare words and find differences\n",
    "            for i, (orig, corr) in enumerate(zip(original_words, corrected_words)):\n",
    "                if orig.lower() != corr.lower():\n",
    "                    # Get word tags for context\n",
    "                    tags = dict(sentence_blob.tags)\n",
    "                    word_type = tags.get(orig, 'unknown')\n",
    "                    \n",
    "                    correction = {\n",
    "                        'word': orig,\n",
    "                        'suggestion': corr,\n",
    "                        'position': word_position + i,\n",
    "                        'word_type': word_type,\n",
    "                        'confidence': self._calculate_confidence(orig, corr)\n",
    "                    }\n",
    "                    corrections.append(correction)\n",
    "            \n",
    "            word_position += len(original_words)\n",
    "        \n",
    "        return corrections\n",
    "    \n",
    "    def _calculate_confidence(self, original, correction):\n",
    "        \"\"\"\n",
    "        Calculate a simple confidence score for the correction\n",
    "        \"\"\"\n",
    "        # Calculate Levenshtein distance\n",
    "        distance = self._levenshtein_distance(original.lower(), correction.lower())\n",
    "        max_len = max(len(original), len(correction))\n",
    "        \n",
    "        # Convert to a confidence score (0-100)\n",
    "        confidence = (1 - (distance / max_len)) * 100\n",
    "        return round(confidence, 2)\n",
    "    \n",
    "    def _levenshtein_distance(self, s1, s2):\n",
    "        \"\"\"\n",
    "        Calculate the Levenshtein distance between two strings\n",
    "        \"\"\"\n",
    "        if len(s1) < len(s2):\n",
    "            return self._levenshtein_distance(s2, s1)\n",
    "\n",
    "        if len(s2) == 0:\n",
    "            return len(s1)\n",
    "\n",
    "        previous_row = range(len(s2) + 1)\n",
    "        for i, c1 in enumerate(s1):\n",
    "            current_row = [i + 1]\n",
    "            for j, c2 in enumerate(s2):\n",
    "                insertions = previous_row[j + 1] + 1\n",
    "                deletions = current_row[j] + 1\n",
    "                substitutions = previous_row[j] + (c1 != c2)\n",
    "                current_row.append(min(insertions, deletions, substitutions))\n",
    "            previous_row = current_row\n",
    "\n",
    "        return previous_row[-1]\n",
    "    \n",
    "    def print_corrections(self, text):\n",
    "        \"\"\"\n",
    "        Print corrections in a formatted way\n",
    "        \"\"\"\n",
    "        try:\n",
    "            corrections = self.check_text(text)\n",
    "            \n",
    "            if not corrections:\n",
    "                print(\"No spelling or grammar issues found!\")\n",
    "                return\n",
    "            \n",
    "            print(f\"Found {len(corrections)} potential issues:\\n\")\n",
    "            \n",
    "            for i, correction in enumerate(corrections, 1):\n",
    "                print(f\"Issue #{i}:\")\n",
    "                print(f\"- Word: {correction['word']}\")\n",
    "                print(f\"- Suggestion: {correction['suggestion']}\")\n",
    "                print(f\"- Position: word #{correction['position']}\")\n",
    "                print(f\"- Word Type: {correction['word_type']}\")\n",
    "                print(f\"- Confidence: {correction['confidence']}%\")\n",
    "                print()\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"An error occurred while checking the text: {str(e)}\")\n",
    "\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    checker = TextBlobChecker()\n",
    "    \n",
    "    test_text = \"\"\"The deadlist virus in modern history, perhaps of all time, was the 1918 Spanish Flu. \n",
    "    It killed about 20 to 50 million people worldwide, perhaps more. \n",
    "    The total death toll is unknown because medical records were not kept in many areas.\"\"\"\n",
    "    \n",
    "    print(\"Checking text:\")\n",
    "    print(test_text)\n",
    "    print(\"\\nResults:\")\n",
    "    checker.print_corrections(test_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-01-10 15:03:44,658 SequenceTagger predicts: Dictionary with 19 tags: <unk>, NOUN, VERB, PUNCT, ADP, DET, PROPN, PRON, ADJ, ADV, CCONJ, PART, NUM, AUX, INTJ, SYM, X, <START>, <STOP>\n",
      "Sentence[4]: \"I love Berlin.\" → [\"I\"/PRON, \"love\"/VERB, \"Berlin\"/PROPN, \".\"/PUNCT]\n",
      "The following NER tags are found:\n"
     ]
    }
   ],
   "source": [
    "from flair.data import Sentence\n",
    "from flair.models import SequenceTagger\n",
    "\n",
    "# load tagger\n",
    "tagger = SequenceTagger.load(\"flair/upos-english\")\n",
    "\n",
    "# make example sentence\n",
    "sentence = Sentence(\"I love Berlin.\")\n",
    "\n",
    "# predict NER tags\n",
    "tagger.predict(sentence)\n",
    "\n",
    "# print sentence\n",
    "print(sentence)\n",
    "\n",
    "# print predicted NER spans\n",
    "print('The following NER tags are found:')\n",
    "# iterate over entities and print\n",
    "for entity in sentence.get_spans('pos'):\n",
    "    print(entity)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Dependency Parse:\n",
      "Jack         nsubj        messed\n",
      "messed       ROOT         messed\n",
      "up           prt          messed\n",
      "his          poss         room\n",
      "room         dobj         messed\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<span class=\"tex2jax_ignore\"><svg xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\" xml:lang=\"en\" id=\"7a8ed4786e80462a904edcb19d754a26-0\" class=\"displacy\" width=\"650\" height=\"242.0\" direction=\"ltr\" style=\"max-width: none; height: 242.0px; color: #000000; background: #ffffff; font-family: Arial; direction: ltr\">\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"162.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"50\">Jack</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"50\">PROPN</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"162.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"170\">messed</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"170\">VERB</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"162.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"290\">up</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"290\">ADP</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"162.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"410\">his</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"410\">PRON</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"162.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"530\">room</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"530\">NOUN</tspan>\n",
       "</text>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-7a8ed4786e80462a904edcb19d754a26-0-0\" stroke-width=\"2px\" d=\"M70,122.0 C70,62.0 165.0,62.0 165.0,122.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-7a8ed4786e80462a904edcb19d754a26-0-0\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">nsubj</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M70,124.0 L70,120.0 70,120.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-7a8ed4786e80462a904edcb19d754a26-0-1\" stroke-width=\"2px\" d=\"M190,122.0 C190,62.0 285.0,62.0 285.0,122.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-7a8ed4786e80462a904edcb19d754a26-0-1\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">prt</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M285.0,124.0 L285.0,120.0 285.0,120.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-7a8ed4786e80462a904edcb19d754a26-0-2\" stroke-width=\"2px\" d=\"M430,122.0 C430,62.0 525.0,62.0 525.0,122.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-7a8ed4786e80462a904edcb19d754a26-0-2\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">poss</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M430,124.0 L430,120.0 430,120.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-7a8ed4786e80462a904edcb19d754a26-0-3\" stroke-width=\"2px\" d=\"M190,122.0 C190,2.0 530.0,2.0 530.0,122.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-7a8ed4786e80462a904edcb19d754a26-0-3\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">dobj</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M530.0,124.0 L530.0,120.0 530.0,120.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "</svg></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "TypeError",
     "evalue": "write() argument must be str, not None",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 45\u001b[0m\n\u001b[0;32m     32\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msyntax_tree.html\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mw\u001b[39m\u001b[38;5;124m\"\u001b[39m, encoding\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[0;32m     33\u001b[0m     f\u001b[38;5;241m.\u001b[39mwrite(\u001b[38;5;124m\"\"\"\u001b[39m\n\u001b[0;32m     34\u001b[0m \u001b[38;5;124m    <html>\u001b[39m\n\u001b[0;32m     35\u001b[0m \u001b[38;5;124m    <head>\u001b[39m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     43\u001b[0m \u001b[38;5;124m        <div class=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msyntax-tree\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m>\u001b[39m\n\u001b[0;32m     44\u001b[0m \u001b[38;5;124m    \u001b[39m\u001b[38;5;124m\"\"\"\u001b[39m)\n\u001b[1;32m---> 45\u001b[0m     \u001b[43mf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwrite\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhtml\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     46\u001b[0m     f\u001b[38;5;241m.\u001b[39mwrite(\u001b[38;5;124m\"\"\"\u001b[39m\n\u001b[0;32m     47\u001b[0m \u001b[38;5;124m        </div>\u001b[39m\n\u001b[0;32m     48\u001b[0m \u001b[38;5;124m    </body>\u001b[39m\n\u001b[0;32m     49\u001b[0m \u001b[38;5;124m    </html>\u001b[39m\n\u001b[0;32m     50\u001b[0m \u001b[38;5;124m    \u001b[39m\u001b[38;5;124m\"\"\"\u001b[39m)\n\u001b[0;32m     52\u001b[0m \u001b[38;5;66;03m# Print detailed syntactic analysis\u001b[39;00m\n",
      "\u001b[1;31mTypeError\u001b[0m: write() argument must be str, not None"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "from spacy import displacy\n",
    "\n",
    "# Load English language model\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# Process the text\n",
    "doc = nlp(\"Jack messed up his room\")\n",
    "\n",
    "# Print dependency information\n",
    "print(\"\\nDependency Parse:\")\n",
    "for token in doc:\n",
    "    print(f\"{token.text:12} {token.dep_:12} {token.head.text}\")\n",
    "\n",
    "# Configure visualization options\n",
    "options = {\n",
    "    \"compact\": False,\n",
    "    \"bg\": \"#ffffff\",\n",
    "    \"color\": \"#000000\",\n",
    "    \"font\": \"Arial\",\n",
    "    \"arrow_spacing\": 20,\n",
    "    \"arrow_width\": 2,\n",
    "    \"distance\": 120,\n",
    "    \"offset_x\": 50,\n",
    "    \"word_spacing\": 40,\n",
    "}\n",
    "\n",
    "# Generate and display syntax tree visualization\n",
    "html = displacy.render(doc, style=\"dep\", options=options)\n",
    "\n",
    "# Save visualization to file\n",
    "with open(\"syntax_tree.html\", \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(\"\"\"\n",
    "    <html>\n",
    "    <head>\n",
    "        <title>Syntax Tree</title>\n",
    "        <style>\n",
    "            body { margin: 20px; }\n",
    "            .syntax-tree { border: 1px solid #ccc; padding: 20px; }\n",
    "        </style>\n",
    "    </head>\n",
    "    <body>\n",
    "        <div class=\"syntax-tree\">\n",
    "    \"\"\")\n",
    "    f.write(html)\n",
    "    f.write(\"\"\"\n",
    "        </div>\n",
    "    </body>\n",
    "    </html>\n",
    "    \"\"\")\n",
    "\n",
    "# Print detailed syntactic analysis\n",
    "print(\"\\nDetailed Syntactic Analysis:\")\n",
    "for token in doc:\n",
    "    print(f\"\"\"\n",
    "Token: {token.text}\n",
    "    Dependency: {token.dep_}\n",
    "    Head word: {token.head.text}\n",
    "    Part of speech: {token.pos_}\n",
    "    Syntactic tag: {token.tag_}\n",
    "    Detailed tag: {spacy.explain(token.tag_)}\n",
    "    Dependency explanation: {spacy.explain(token.dep_)}\n",
    "    Children: {[child.text for child in token.children]}\n",
    "    \"\"\")\n",
    "\n",
    "# Print phrase structure\n",
    "print(\"\\nPhrase Structure:\")\n",
    "for chunk in doc.noun_chunks:\n",
    "    print(f\"\"\"\n",
    "Noun Phrase: {chunk.text}\n",
    "    Root text: {chunk.root.text}\n",
    "    Root dep_: {chunk.root.dep_}\n",
    "    Root head text: {chunk.root.head.text}\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['C:\\\\Users\\\\LEGION/nltk_data', 'c:\\\\Users\\\\LEGION\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\Python312\\\\nltk_data', 'c:\\\\Users\\\\LEGION\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\Python312\\\\share\\\\nltk_data', 'c:\\\\Users\\\\LEGION\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\Python312\\\\lib\\\\nltk_data', 'C:\\\\Users\\\\LEGION\\\\AppData\\\\Roaming\\\\nltk_data', 'C:\\\\nltk_data', 'D:\\\\nltk_data', 'E:\\\\nltk_data']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\LEGION\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\LEGION\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    },
    {
     "ename": "LookupError",
     "evalue": "\n**********************************************************************\n  Resource \u001b[93mpunkt_tab\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('punkt_tab')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mtokenizers/punkt_tab/english/\u001b[0m\n\n  Searched in:\n    - 'C:\\\\Users\\\\LEGION/nltk_data'\n    - 'c:\\\\Users\\\\LEGION\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\Python312\\\\nltk_data'\n    - 'c:\\\\Users\\\\LEGION\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\Python312\\\\share\\\\nltk_data'\n    - 'c:\\\\Users\\\\LEGION\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\Python312\\\\lib\\\\nltk_data'\n    - 'C:\\\\Users\\\\LEGION\\\\AppData\\\\Roaming\\\\nltk_data'\n    - 'C:\\\\nltk_data'\n    - 'D:\\\\nltk_data'\n    - 'E:\\\\nltk_data'\n**********************************************************************\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mLookupError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 12\u001b[0m\n\u001b[0;32m      9\u001b[0m sample_text \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe quick brown fox jumps over the lazy dog\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     11\u001b[0m \u001b[38;5;66;03m# Find all parts of speech in above sentence\u001b[39;00m\n\u001b[1;32m---> 12\u001b[0m tagged \u001b[38;5;241m=\u001b[39m pos_tag(\u001b[43mword_tokenize\u001b[49m\u001b[43m(\u001b[49m\u001b[43msample_text\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[0;32m     14\u001b[0m \u001b[38;5;66;03m#Extract all parts of speech from any text\u001b[39;00m\n\u001b[0;32m     15\u001b[0m chunker \u001b[38;5;241m=\u001b[39m RegexpParser(\u001b[38;5;124m\"\"\"\u001b[39m\n\u001b[0;32m     16\u001b[0m \u001b[38;5;124m                       NP: \u001b[39m\u001b[38;5;124m{\u001b[39m\u001b[38;5;124m<DT>?<JJ>*<NN>}    #To extract Noun Phrases\u001b[39m\n\u001b[0;32m     17\u001b[0m \u001b[38;5;124m                       P: \u001b[39m\u001b[38;5;124m{\u001b[39m\u001b[38;5;124m<IN>}               #To extract Prepositions\u001b[39m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     22\u001b[0m \u001b[38;5;124m                       VP: \u001b[39m\u001b[38;5;124m{\u001b[39m\u001b[38;5;124m<V> <NP|PP>*}      #To extract Verb Phrases\u001b[39m\n\u001b[0;32m     23\u001b[0m \u001b[38;5;124m                       \u001b[39m\u001b[38;5;124m\"\"\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\LEGION\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\nltk\\tokenize\\__init__.py:142\u001b[0m, in \u001b[0;36mword_tokenize\u001b[1;34m(text, language, preserve_line)\u001b[0m\n\u001b[0;32m    127\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mword_tokenize\u001b[39m(text, language\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124menglish\u001b[39m\u001b[38;5;124m\"\u001b[39m, preserve_line\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[0;32m    128\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    129\u001b[0m \u001b[38;5;124;03m    Return a tokenized copy of *text*,\u001b[39;00m\n\u001b[0;32m    130\u001b[0m \u001b[38;5;124;03m    using NLTK's recommended word tokenizer\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    140\u001b[0m \u001b[38;5;124;03m    :type preserve_line: bool\u001b[39;00m\n\u001b[0;32m    141\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 142\u001b[0m     sentences \u001b[38;5;241m=\u001b[39m [text] \u001b[38;5;28;01mif\u001b[39;00m preserve_line \u001b[38;5;28;01melse\u001b[39;00m \u001b[43msent_tokenize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlanguage\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    143\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [\n\u001b[0;32m    144\u001b[0m         token \u001b[38;5;28;01mfor\u001b[39;00m sent \u001b[38;5;129;01min\u001b[39;00m sentences \u001b[38;5;28;01mfor\u001b[39;00m token \u001b[38;5;129;01min\u001b[39;00m _treebank_word_tokenizer\u001b[38;5;241m.\u001b[39mtokenize(sent)\n\u001b[0;32m    145\u001b[0m     ]\n",
      "File \u001b[1;32mc:\\Users\\LEGION\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\nltk\\tokenize\\__init__.py:119\u001b[0m, in \u001b[0;36msent_tokenize\u001b[1;34m(text, language)\u001b[0m\n\u001b[0;32m    109\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21msent_tokenize\u001b[39m(text, language\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124menglish\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m    110\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    111\u001b[0m \u001b[38;5;124;03m    Return a sentence-tokenized copy of *text*,\u001b[39;00m\n\u001b[0;32m    112\u001b[0m \u001b[38;5;124;03m    using NLTK's recommended sentence tokenizer\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    117\u001b[0m \u001b[38;5;124;03m    :param language: the model name in the Punkt corpus\u001b[39;00m\n\u001b[0;32m    118\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 119\u001b[0m     tokenizer \u001b[38;5;241m=\u001b[39m \u001b[43m_get_punkt_tokenizer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlanguage\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    120\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m tokenizer\u001b[38;5;241m.\u001b[39mtokenize(text)\n",
      "File \u001b[1;32mc:\\Users\\LEGION\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\nltk\\tokenize\\__init__.py:105\u001b[0m, in \u001b[0;36m_get_punkt_tokenizer\u001b[1;34m(language)\u001b[0m\n\u001b[0;32m     96\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mlru_cache\n\u001b[0;32m     97\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_get_punkt_tokenizer\u001b[39m(language\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124menglish\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m     98\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m     99\u001b[0m \u001b[38;5;124;03m    A constructor for the PunktTokenizer that utilizes\u001b[39;00m\n\u001b[0;32m    100\u001b[0m \u001b[38;5;124;03m    a lru cache for performance.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    103\u001b[0m \u001b[38;5;124;03m    :type language: str\u001b[39;00m\n\u001b[0;32m    104\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 105\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mPunktTokenizer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlanguage\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\LEGION\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\nltk\\tokenize\\punkt.py:1744\u001b[0m, in \u001b[0;36mPunktTokenizer.__init__\u001b[1;34m(self, lang)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, lang\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124menglish\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m   1743\u001b[0m     PunktSentenceTokenizer\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m)\n\u001b[1;32m-> 1744\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_lang\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlang\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\LEGION\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\nltk\\tokenize\\punkt.py:1749\u001b[0m, in \u001b[0;36mPunktTokenizer.load_lang\u001b[1;34m(self, lang)\u001b[0m\n\u001b[0;32m   1746\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mload_lang\u001b[39m(\u001b[38;5;28mself\u001b[39m, lang\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124menglish\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m   1747\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mnltk\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m find\n\u001b[1;32m-> 1749\u001b[0m     lang_dir \u001b[38;5;241m=\u001b[39m \u001b[43mfind\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtokenizers/punkt_tab/\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mlang\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m/\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1750\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_params \u001b[38;5;241m=\u001b[39m load_punkt_params(lang_dir)\n\u001b[0;32m   1751\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lang \u001b[38;5;241m=\u001b[39m lang\n",
      "File \u001b[1;32mc:\\Users\\LEGION\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\nltk\\data.py:579\u001b[0m, in \u001b[0;36mfind\u001b[1;34m(resource_name, paths)\u001b[0m\n\u001b[0;32m    577\u001b[0m sep \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m*\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m70\u001b[39m\n\u001b[0;32m    578\u001b[0m resource_not_found \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00msep\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mmsg\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00msep\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m--> 579\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mLookupError\u001b[39;00m(resource_not_found)\n",
      "\u001b[1;31mLookupError\u001b[0m: \n**********************************************************************\n  Resource \u001b[93mpunkt_tab\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('punkt_tab')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mtokenizers/punkt_tab/english/\u001b[0m\n\n  Searched in:\n    - 'C:\\\\Users\\\\LEGION/nltk_data'\n    - 'c:\\\\Users\\\\LEGION\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\Python312\\\\nltk_data'\n    - 'c:\\\\Users\\\\LEGION\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\Python312\\\\share\\\\nltk_data'\n    - 'c:\\\\Users\\\\LEGION\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\Python312\\\\lib\\\\nltk_data'\n    - 'C:\\\\Users\\\\LEGION\\\\AppData\\\\Roaming\\\\nltk_data'\n    - 'C:\\\\nltk_data'\n    - 'D:\\\\nltk_data'\n    - 'E:\\\\nltk_data'\n**********************************************************************\n"
     ]
    }
   ],
   "source": [
    "# Import required libraries\n",
    "import nltk\n",
    "print(nltk.data.path)\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "from nltk import pos_tag, word_tokenize, RegexpParser\n",
    "  \n",
    "# Example text\n",
    "sample_text = \"The quick brown fox jumps over the lazy dog\"\n",
    "  \n",
    "# Find all parts of speech in above sentence\n",
    "tagged = pos_tag(word_tokenize(sample_text))\n",
    "  \n",
    "#Extract all parts of speech from any text\n",
    "chunker = RegexpParser(\"\"\"\n",
    "                       NP: {<DT>?<JJ>*<NN>}    #To extract Noun Phrases\n",
    "                       P: {<IN>}               #To extract Prepositions\n",
    "                       V: {<V.*>}              #To extract Verbs\n",
    "                       PP: {\n",
    " \n",
    "<p> <NP>}          #To extract Prepositional Phrases\n",
    "                       VP: {<V> <NP|PP>*}      #To extract Verb Phrases\n",
    "                       \"\"\")\n",
    " \n",
    "# Print all parts of speech in above sentence\n",
    "output = chunker.parse(tagged)\n",
    "print(\"After Extracting\\n\", output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
